---
output:
  html_document: default
  word_document: default
  pdf_document: default
---
title: 'MAM 2022 Pre-programme Assignment'
author: "Valeria Opre"
date: "27.08.2021"
output:
  html_document:
  theme: flatly
highlight: zenburn
toc: yes
toc_float: yes
---
---


```{r load-libraries, warning=FALSE, message=FALSE, echo=FALSE}
library(tidyverse)  # Load ggplot2, dplyr, and all the other tidyverse packages
library(gapminder)  # gapminder dataset
library(here)
```


> Please delete all the intro text I wrote from line 22 to line 69 and start writing your short biography after this blockquote.

## Task 1: Who am I?
\
My name is __Valeria Opre__, I am 24 years old and I grew up in the German speaking region of Italy, South Tyrol. I went to the Italian highschool _Liceo Scientifico Torricelli_.
\
I decided to move out and live in Vienna, where i first graduated in _Political Science_ and then completed my second degree with honours in _Business, Economics and Social Sciences_ at the Vienna University of Economics and Business. During this time I had the great chance to do an exchange semester at the SMU in Singapore.

Before applying for my postgraduate studies in London I decided to pursue two internships to better shape my future goals: 
  + As a _research consultant_ for __Action Institute__, a renowned economic and political NGO in Milan
  + As an _equity analyst_ for __Wutis__, winning two pitching competition with McKinsey.
 
Arriving to London for my Masters at LBS I am excited to learn more programming languages and how to use them to make more strategic business decisions!

If you are interested in my profile, you can look at my [linkedin profile](https://www.linkedin.com/in/valeriaopre/).

![](https://upload.wikimedia.org/wikipedia/commons/4/47/RS9327_LBS_Standard_Logo_RGB_AW-hpr.jpg)


# Task 2: `gapminder` country comparison

You have seen the `gapminder` dataset that has data on life expectancy, population, and GDP per capita for 142 countries from 1952 to 2007. To get a glipmse of the dataframe, namely to see the variable names, variable types, etc., we use the `glimpse` function. We also want to have a look at the first 20 rows of data.

```{r}
glimpse(gapminder)

head(gapminder, 20) # look at the first 20 rows of the dataframe
```

Your task is to produce two graphs of how life expectancy has changed over the years for the `country` and the `continent` you come from. 

I have created the `country_data` and `continent_data` with the code below. 

```{r}
country_data <- gapminder %>% 
            filter(country == "Italy")

continent_data <- gapminder %>% 
            filter(continent == "Europe")
```

First, create a plot of life expectancy over time for the single country you chose. You should use  `geom_point()` to see the actual data points and `geom_smooth(se = FALSE)`  to plot the underlying trendlines. You need to remove the comments **#** from the lines below for your code to run.

```{r, lifeExp_one_country}
plot1 <- ggplot(country_data, mapping = aes(x = year, y = lifeExp))+
   geom_point() +
   geom_smooth(se = FALSE)+
   NULL 

plot1
```

Next we need to add a title. Create a new plot, or extend plot1, using the `labs()` function to add an informative title to the plot.

```{r, lifeExp_one_country_with_label}
 
 plot1<- plot1 +
   labs(title = "Life expectancy over time in Italy ",
       x = " Years",
       y = "Life Expectancy") +
   NULL

print(plot1)
```


Secondly, produce a plot for all countries in the *continent* you come from. (Hint: map the `country` variable to the colour aesthetic. You also want to map `country` to the `group` aesthetic, so all points for each country are grouped together).
  
```{r lifeExp_one_continent}
 ggplot(data= continent_data, mapping = aes(x = year , y = lifeExp , colour= country, group = country))+
   geom_point() + 
   geom_smooth(se = FALSE) +
 NULL
```


Finally, using the original `gapminder` data, produce a life expectancy over time graph, grouped (or faceted) by continent. We will remove all legends, adding the `theme(legend.position="none")` in the end of our ggplot.

```{r lifeExp_facet_by_continent}
 ggplot(data = gapminder , mapping = aes(x = year , y = lifeExp , colour= country))+
   geom_point() + 
   geom_smooth(se = FALSE) +
   facet_wrap(~continent) +
   theme(legend.position="none") +  #remove all legends
   NULL
```

Given these trends, what can you say about life expectancy since 1952? Again, don't just say what's happening in the graph. Tell some sort of story and speculate about the differences in the patterns.

> Type your answer after this blockquote.

### Italy\
The graph shows the steady increase in Life expectancy in Italy starting from 1952. 
Nowadays, Italians are expected to live more than 80 years on average in comparison to less than 65 years in 1952 on average. 
One of the reasons that could explain a higher life expectancy is an increase in medical assistance and better living standards.


### Europe\
Since 1952, every European country's life expectancy has increased. Even though the increase in some countries is greater than in others, the gap between European countries is now narrower than it was in 1952.
As a result, some of the countries with the lowest life expectancy in 1952 have shown a greater increase in life expectancy over time than countries with already higher life expectancy  One reason for this could be that these countries  have been underdeveloped in terms of access to medical aid and living standards and have exploited the knowledge and assistance of other countries strongly improving life expectancy.
Countries with higher life expectancy in 1952, on the other hand, could primarily increase their life expectancy through new research and findings. Furthermore, once a certain standard is reached, it appears difficult to increase life expectancy further.

### Life expectancy over time\
Overall, life expectancy is rising, but there are some regional differences. While life expectancy is increasing in every country in the Americas, Oceania, and Europe, and the gap between countries is closing, it is widening in Africa and Asia.
This could be explained from stronger polarization between the development of countries within Asia and Africa: some countries in Asia and Africa are very underdeveloped or do not have access to the same financial aid as other countries, whereas other countries have made significant progress over the years. Another factor that may have reduced life expectancy in some countries is war, disease, or political instability.

# Task 3: Brexit voting

We will have a quick look at the results of the 2016 Brexit vote in the UK. First we read the data using `read_csv()` and have a quick glimpse at the data

```{r load_brexit_data, message=FALSE, warning=FALSE}
brexit_results <- read_csv("https://assets.datacamp.com/production/repositories/1934/datasets/7c5ad33c949eb0042d50f8c18d538cde0c7bf4e7/brexit_results.csv")
show_col_types = FALSE

glimpse(brexit_results)
```

glimpse(brexit_results)
The data comes from [Elliott Morris](https://www.thecrosstab.com/), who cleaned it and made it available through his [DataCamp class on analysing election and polling data in R](https://www.datacamp.com/courses/analyzing-election-and-polling-data-in-r).

Our main outcome variable (or y) is `leave_share`, which is the percent of votes cast in favour of Brexit, or leaving the EU. Each row is a UK [parliament constituency](https://en.wikipedia.org/wiki/United_Kingdom_Parliament_constituencies). 

To get a sense of the spread, or distribution, of the data, we can plot a histogram, a density plot, and the empirical cumulative distribution function of the leave % in all constituencies.

```{r brexit_histogram, warning=FALSE, message=FALSE}
# histogram
ggplot(brexit_results, aes(x = leave_share)) +
  geom_histogram(binwidth = 2.5)
# density plot-- think smoothed histogram
ggplot(brexit_results, aes(x = leave_share)) +
  geom_density()
# The empirical cumulative distribution function (ECDF) 
ggplot(brexit_results, aes(x = leave_share)) +
  stat_ecdf(geom = "step", pad = FALSE) +
  scale_y_continuous(labels = scales::percent)

```

One common explanation for the Brexit outcome was fear of immigration and opposition to the EU's more open border policy. We can check the relationship (or correlation) between the proportion of native born residents (`born_in_uk`) in a constituency and its `leave_share`. To do this, let us get the correlation between the two variables

```{r brexit_immigration_correlation}
brexit_results %>% 
  select("leave share" = leave_share , "born in UK" = born_in_uk) %>% 
  cor()
```

The correlation is almost 0.5, which shows that the two variables are positively correlated. 

We can also create a scatterplot between these two variables using `geom_point`. We also add the best fit line, using  `geom_smooth(method = "lm")`. 

```{r brexit_immigration_plot}
ggplot(brexit_results, aes(x = born_in_uk, y = leave_share)) +
  geom_point(alpha=0.3) +
  geom_smooth(method = "lm") +
  theme_bw() +
  labs(title= "Brexit Vote 2016",
       subtitle="Correlation between leave vote and native born residents",
       x= "Native born residents",
       y= "Leave share") +
  NULL
```

You have the code for the plots, I would like you to revisit all of them and use the `labs()` function to add an informative title, subtitle, and axes titles to all plots.

What can you say about the relationship shown above? Again, don't just say what's happening in the graph. Tell some sort of story and speculate about the differences in the patterns.

> Type your answer after, and outside, this blockquote.

The scatter plot shows the strong relationship between local residents and the voting shares they drop. The speculations mentioned above about immigration and opposition to the EU's more open border policy appear to be correct.

# Task 4: Animal rescue incidents attended by the London Fire Brigade

[The London Fire Brigade](https://data.london.gov.uk/dataset/animal-rescue-incidents-attended-by-lfb) attends a range of non-fire incidents (which we call 'special services'). These 'special services' include assistance to animals that may be trapped or in distress. The data is provided from January 2009 and is updated monthly. A range of information is supplied for each incident including some location information (postcode, borough, ward), as well as the data/time of the incidents. We do not routinely record data about animal deaths or injuries.

Please note that any cost included is a notional cost calculated based on the length of time rounded up to the nearest hour spent by Pump, Aerial and FRU appliances at the incident and charged at the current Brigade hourly rate.

```{r load_animal_rescue_data, warning=FALSE, message=FALSE}
url <- "https://data.london.gov.uk/download/animal-rescue-incidents-attended-by-lfb/8a7d91c2-9aec-4bde-937a-3998f4717cd8/Animal%20Rescue%20incidents%20attended%20by%20LFB%20from%20Jan%202009.csv"
animal_rescue <- read_csv(url,
                          locale = locale(encoding = "CP1252")) %>% 
  janitor::clean_names()

glimpse(animal_rescue)
```
One of the more useful things one can do with any data set is quick counts, namely to see how many observations fall within one category. For instance, if we wanted to count the number of incidents by year, we would either use `group_by()... summarise()` or, simply [`count()`](https://dplyr.tidyverse.org/reference/count.html)

```{r, instances_by_calendar_year}
animal_rescue %>% 
  dplyr::group_by(cal_year) %>% 
  summarise(count=n())
animal_rescue %>% 
  count(cal_year, name="count")
```

Let us try to see how many incidents we have by animal group. Again, we can do this either using group_by() and summarise(), or by using count()

```{r, animal_group_percentages}
animal_rescue %>% 
  group_by(animal_group_parent) %>% 
  
  #group_by and summarise will produce a new column with the count in each animal group
  summarise(count = n()) %>% 
  
  # mutate adds a new column; here we calculate the percentage
  mutate(percent = round(100*count/sum(count),2)) %>% 
  
  # arrange() sorts the data by percent. Since the default sorting is min to max and we would like to see it sorted
  # in descending order (max to min), we use arrange(desc()) 
  arrange(desc(percent))
animal_rescue %>% 
  
  #count does the same thing as group_by and summarise
  # name = "count" will call the column with the counts "count" ( exciting, I know)
  # and 'sort=TRUE' will sort them from max to min
  count(animal_group_parent, name="count", sort=TRUE) %>% 
  mutate(percent = round(100*count/sum(count),2))
```

Do you see anything strange in these tables? 

The first graph depicts the number of incidents by year: there is no clear ascending or descending trend for the number of accidents. On the other hand,
the second graph refers to the relative division of such accidents regarding the animal group. What becomes clear is that the most endangered animals are
cats, making up for more than 48% of all accidents. Birds and Dogs are also victims of the accidents in a greater percentage than the other spiecies.


Finally, let us have a loot at the notional cost for rescuing each of these animals. As the LFB says,

> Please note that any cost included is a notional cost calculated based on the length of time rounded up to the nearest hour spent by Pump, Aerial and FRU appliances at the incident and charged at the current Brigade hourly rate.
There is two things we will do:

1. Calculate the mean and median `incident_notional_cost` for each `animal_group_parent`
2. Plot a boxplot to get a feel for the distribution of `incident_notional_cost` by `animal_group_parent`.



Before we go on, however, we need to fix `incident_notional_cost` as it is stored as a `chr`, or character, rather than a number.

```{r, parse_incident_cost,message=FALSE, warning=FALSE}
# what type is variable incident_notional_cost from dataframe `animal_rescue`
typeof(animal_rescue$incident_notional_cost)
# readr::parse_number() will convert any numerical values stored as characters into numbers
animal_rescue <- animal_rescue %>% 
  # we use mutate() to use the parse_number() function and overwrite the same variable
  mutate(incident_notional_cost = parse_number(incident_notional_cost))
# incident_notional_cost from dataframe `animal_rescue` is now 'double' or numeric
typeof(animal_rescue$incident_notional_cost)

```
```

Compare the mean and the median for each animal group. what do you think this is telling us?
Anything else that stands out? Any outliers?


Finally, let us plot a few plots that show the distribution of incident_cost for each animal group.

```{r, plots_on_incident_cost_by_animal_group,message=FALSE, warning=FALSE}
# base_plot
base_plot <- animal_rescue %>% 
  group_by(animal_group_parent) %>% 
  filter(n()>6) %>% 
  ggplot(aes(x=incident_notional_cost))+
  facet_wrap(~animal_group_parent, scales = "free")+
  theme_bw()
base_plot + geom_histogram()
base_plot + geom_density()
base_plot + geom_boxplot()
base_plot + stat_ecdf(geom = "step", pad = FALSE) +
  scale_y_continuous(labels = scales::percent)
```

Which of these four graphs do you think best communicates the variability of the `incident_notional_cost` values? Also, can you please tell some sort of story (which animals are more expensive to rescue than others, the spread of values) and speculate about the differences in the patterns.

Variability of the values will reflect by how much the data points in the distribution  diverge from the average, as well as the extent to which these data points differ from each other.
The first graph (histograph), only allows to roughfly gain an understanding about the different incident costs per animal group. Flor instance, it becomes clear how horses are more expensive to rescue and also
do not have that high number of accidents. On the other hand, rescuing birds seems to be less expensive. The density plot expresses the same concept as the histogram. Generally it shows that  less expensive the rescue becomes, the more probable the rescue itself. This does not hold true for the Rabbit and Ferret. More interesting is the depiction of variability of the scatter plot (third graph). 


# Submit the assignment

Knit the completed R Markdown file as an HTML document (use the "Knit" button at the top of the script editor window) and upload it to Canvas.


## Details

If you want to, please answer the following

- Who did you collaborate with: NOBODY
- Approximately how much time did you spend on this problem set: 10 hours
- What, if anything, gave you the most trouble: I had some truble downloading the data (brexit); I tried to overcome this by searching for the data (since you provided us with the source) and uploading it that way. Moreover, for Task4, I was not able to fully complete it since I got stuck with the "mutate" function and could not find a solution to further analyse the mean and median of the incident_notional_cost.
